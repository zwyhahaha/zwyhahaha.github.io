<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Over the Rainbow</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 07 Feb 2026 20:46:30 -0800</pubDate>
    <lastBuildDate>Sat, 07 Feb 2026 20:46:30 -0800</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>Polynomial Identity Testing and Schwartz-Zippel Lemma</title>
        <description>&lt;p&gt;Given two polynomials $f$ and $g$, like $x_1^2 - x_2^2$ and $(x_1 + x_2)(x_1 - x_2)$, how do we determine whether they are equal? This problem is known as &lt;em&gt;Polynomial Identity Testing&lt;/em&gt; (PIT), which is equivalent to testing whether $f - g$ is identically zero. A polynomial is identically $0$ if and only if all its coefficients are $0$. However, expanding a polynomial into its monomial representation may incur exponential cost. This blog presents a simple randomized algorithm that solves PIT efficiently.&lt;/p&gt;

&lt;p&gt;I learned this algorithm in the &lt;em&gt;Combinatorial Optimization&lt;/em&gt; class (MS&amp;amp;E 315) taught by Professor Aaron Sidford. The course encodes the problem of testing whether a graph has a perfect matching as an instance of PIT. (See the &lt;a href=&quot;https://drive.google.com/file/d/1KeqiYijcLzkfmvqs69inXX55WG1-6aK_/view&quot;&gt;lecture notes&lt;/a&gt;, Chapter 10, for details.)&lt;/p&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;

&lt;p&gt;The algorithm is remarkably simple: randomly sample a point and check whether the polynomial evaluates to $0$.&lt;/p&gt;

&lt;p&gt;Specifically, let $f(x_1, \ldots, x_n)$ be a polynomial of degree at most $d$ over a field $\mathbb{F}$, and let $S \subseteq \mathbb{F}$ be any finite set. Sample $r_1, \ldots, r_n$ independently and uniformly at random from $S$. If $f(r_1, \ldots, r_n) = 0$, return “$f \equiv 0$”; otherwise return “$f \not\equiv 0$”.&lt;/p&gt;

&lt;p&gt;The correctness of this algorithm is guaranteed by the following lemma:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Schwartz-Zippel Lemma.&lt;/strong&gt; Let $f \in \mathbb{F}[x_1, \ldots, x_n]$ be a nonzero polynomial of degree at most $d$. For any finite set $S \subseteq \mathbb{F}$, if $r_1, \ldots, r_n$ are sampled independently and uniformly at random from $S$, then&lt;/p&gt;

\[\Pr[f(r_1, \ldots, r_n) = 0] \leq \frac{d}{\lvert S\rvert}.\]
&lt;/blockquote&gt;

&lt;p&gt;Several observations follow from this lemma:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The algorithm can only err when $f \not\equiv 0$ but the random evaluation happens to land on a root of $f$. The Schwartz-Zippel Lemma guarantees that the success probability is at least $1 - \frac{d}{\lvert S\rvert}$. Since $S$ is arbitrary, choosing a larger set $S$ increases the success probability.&lt;/li&gt;
  &lt;li&gt;By repeating the test independently $k$ times, the error probability drops to $\left(\frac{d}{\lvert S\rvert}\right)^k$.&lt;/li&gt;
  &lt;li&gt;The bound is independent of the number of variables $n$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proof&quot;&gt;Proof&lt;/h2&gt;

&lt;p&gt;The proof proceeds by induction on $n$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Base case ($n = 1$).&lt;/strong&gt; The polynomial $f$ is univariate of degree at most $d$, so it has at most $d$ roots over $\mathbb{F}$. Therefore,&lt;/p&gt;

\[\Pr[f(r) = 0] \leq \frac{\lvert\{\text{roots of } f\} \cap S\rvert}{\lvert S\rvert} \leq \frac{d}{\lvert S\rvert}.\]

&lt;p&gt;&lt;strong&gt;Inductive step.&lt;/strong&gt; Assume the lemma holds for all nonzero polynomials in $n - 1$ variables. We prove it for $n$ variables. Factor out $x_1$ by writing&lt;/p&gt;

\[f(x_1, \ldots, x_n) = \sum_{i=0}^{k} x_1^i \, f_i(x_2, \ldots, x_n),\]

&lt;p&gt;where $k$ is the largest power of $x_1$ appearing in $f$, so that $f_k \not\equiv 0$. Note that $\deg(f_k) \leq d - k$, since each monomial $x_1^k \cdot m$ in $f$ contributes a monomial $m$ of degree at most $d - k$ to $f_k$.&lt;/p&gt;

&lt;p&gt;Define the event $A = {f_k(r_2, \ldots, r_n) = 0}$. We consider two cases.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Case 1: $A$ occurs.&lt;/strong&gt; Since $f_k$ is a nonzero polynomial in $n - 1$ variables with $\deg(f_k) \leq d - k$, the induction hypothesis gives&lt;/p&gt;

\[\Pr[A] \leq \frac{d - k}{\lvert S\rvert}.\]

&lt;p&gt;&lt;strong&gt;Case 2: $A^c$ occurs.&lt;/strong&gt; Conditioned on $r_2, \ldots, r_n$ such that $f_k(r_2, \ldots, r_n) \neq 0$, the function $g(x_1) = f(x_1, r_2, \ldots, r_n)$ is a nonzero univariate polynomial of degree $k$ in $x_1$. By the base case argument,&lt;/p&gt;

\[\Pr[f(r) = 0 \mid A^c] \leq \frac{k}{\lvert S\rvert}.\]

&lt;p&gt;Combining the two cases by the law of total probability,&lt;/p&gt;

\[\begin{aligned}
\Pr[f(r) = 0]
&amp;amp;= \Pr[A]\,\Pr[f(r) = 0 \mid A] + \Pr[A^c]\,\Pr[f(r) = 0 \mid A^c] \\
&amp;amp;\leq \Pr[A] + \Pr[f(r) = 0 \mid A^c] \\
&amp;amp;\leq \frac{d - k}{\lvert S\rvert} + \frac{k}{\lvert S\rvert} = \frac{d}{\lvert S\rvert}.
\end{aligned}\]

&lt;p&gt;$\blacksquare$&lt;/p&gt;
</description>
        <pubDate>Sat, 07 Feb 2026 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/2026/02/07/PIT/</link>
        <guid isPermaLink="true">http://localhost:4000/2026/02/07/PIT/</guid>
        
        <category>Lecture Notes</category>
        
        
      </item>
    
      <item>
        <title>Asymptotic Memoryless of Markov Chains</title>
        <description>&lt;p&gt;This is the first post in a series of notes taken from the &lt;strong&gt;Stochastic Systems&lt;/strong&gt; class (MS&amp;amp;E 321) taught by Professor Peter Glynn. This series will be updated as the course progresses.&lt;/p&gt;

&lt;p&gt;A fundamental property of Markov chains is that their long-run behavior “forgets” the initial state. More precisely, for a finite, irreducible, aperiodic Markov chain with transition matrix $P$, the $n$-step transition probabilities $P^n(x,y)$ converge to a stationary distribution $\pi(y)$ at a geometric rate. The key idea behind the proof is a clean algebraic decomposition $P^m = \delta \Lambda + (1-\delta)Q$, which separates a “memoryless” rank-1 component $\Lambda$ from a remainder $Q$. This decomposition admits a natural probabilistic interpretation via coupling, and extends to non-stationary transitions, infinite state spaces, and continuous state spaces through Doeblin’s condition.&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;Consider a Markov chain (MC) with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Finite state space: $\lvert S\rvert = d &amp;lt; \infty$&lt;/li&gt;
  &lt;li&gt;Stationary transition matrix $P$&lt;/li&gt;
  &lt;li&gt;$P$ is irreducible and aperiodic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We begin with a technical lemma that connects irreducibility and aperiodicity to a uniform positivity condition on $P^m$.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Lemma 1.&lt;/strong&gt; Let $P$ be the transition matrix of an irreducible Markov chain on a finite state space $S$. Then $P$ is aperiodic if and only if there exists $m \geq 1$ such that $P^m(x,y) &amp;gt; 0$ for all $x, y \in S$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Remark.&lt;/em&gt; This lemma guarantees the existence of $m$ such that $P^m \gg 0$ (all entries strictly positive), which is the starting point of the convergence analysis below. We will revisit the necessity of finiteness in Lemma 1 at the end of this note.&lt;/p&gt;

&lt;h2 id=&quot;convergence-analysis&quot;&gt;Convergence Analysis&lt;/h2&gt;

&lt;p&gt;We first consider the case $m = 1$, i.e., $P \gg 0$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Decomposition of $P$&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let $\Lambda$ be a &lt;strong&gt;stochastic matrix&lt;/strong&gt; ($\Lambda \mathbf{e} = \mathbf{e}$) with &lt;strong&gt;identical rows&lt;/strong&gt;, i.e., $\Lambda = \begin{bmatrix} \lambda \\ \vdots \\ \lambda \end{bmatrix}$ where $\lambda\  \mathbf{e}= 1$.&lt;/p&gt;

&lt;p&gt;Since $P \gg 0$, there exists $\delta \in (0,1)$ such that $P \geq \delta \Lambda$. Define&lt;/p&gt;

\[P = \delta \Lambda + (1 - \delta) Q, \qquad Q = \frac{P - \delta \Lambda}{1 - \delta}.\]

&lt;p&gt;Then $Q \geq 0$, and $Q$ is stochastic: $Q\mathbf{e} = \frac{P\mathbf{e} - \delta \Lambda\mathbf{e}}{1 - \delta}= \mathbf{e}$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Remark.&lt;/em&gt; The parameter $\delta$ measures how much of $P$ can be “explained” by the memoryless component $\Lambda$. A larger $\delta$ means faster mixing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Computing powers of $P$&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A key property: $R\Lambda = \Lambda$ for any stochastic matrix $R$, since $R\Lambda = R \mathbf{e} \lambda = \mathbf{e} \lambda = \Lambda$.&lt;/p&gt;

&lt;p&gt;Expanding $P^2$:&lt;/p&gt;

\[\begin{aligned}
P^2 &amp;amp;= P(\delta \Lambda + (1 - \delta)Q) \\
&amp;amp;= \delta \Lambda + (1 - \delta)PQ \\
&amp;amp;= \delta \Lambda + (1 - \delta)(\delta \Lambda + (1 - \delta)Q)Q \\
&amp;amp;= \delta \Lambda + (1 - \delta)\delta \Lambda Q + (1 - \delta)^2 Q^2
\end{aligned}\]

&lt;p&gt;By induction, the general formula is:&lt;/p&gt;

\[P^n = \sum_{j=0}^{n-1}\delta(1 - \delta)^{j}\Lambda Q^{j} + (1-\delta)^n Q^n.\]

&lt;p&gt;As $n \to \infty$, the remainder $(1-\delta)^n Q^n \to 0$, and:&lt;/p&gt;

\[P^n \to \sum_{j=0}^{\infty} \delta(1 - \delta)^j \Lambda Q^j \quad \triangleq \quad \Pi.\]

&lt;p&gt;&lt;strong&gt;3. Structure of $\Pi$&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\Pi$ is &lt;strong&gt;stochastic&lt;/strong&gt;: since $Q^j\mathbf{e} = \mathbf{e}$ for all $j$, we have $\Lambda Q^j\mathbf{e} = \Lambda\mathbf{e} = \mathbf{e}$, and the geometric series preserves row sums.&lt;/li&gt;
  &lt;li&gt;$\Pi$ has &lt;strong&gt;identical rows&lt;/strong&gt;: since $\Lambda Q^j= \mathbf{e} (\lambda Q^j)$, every term in the series has identical rows.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore $\Pi$ is a rank-1 stochastic matrix whose common row is the stationary distribution $\pi$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Rate of convergence&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Define the matrix operator norm induced by the $\ell_\infty$ vector norm. For any stochastic matrix $P$, $\lVert P\rVert = 1$. Then:&lt;/p&gt;

\[\lVert P^n - \Pi\rVert = O\!\left((1 - \delta)^n\right) \qquad \text{(rate of mixing)}\]

&lt;p&gt;&lt;strong&gt;5. Generalization to $m &amp;gt; 1$&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By Lemma 1, for a finite irreducible aperiodic chain, there exists $m$ such that $P^m \gg 0$. Applying the $m=1$ argument to $P^m$ yields&lt;/p&gt;

\[P^{m\,k} \to \Pi := \sum_{j=0}^\infty \delta(1-\delta)^j \Lambda Q^j,
\qquad
\lVert\,P^{m\,k}-\Pi\,\rVert = O\!\left((1-\delta)^k\right).\]

&lt;p&gt;For general $n$, write $n=km+r$ with $0\le r&amp;lt;m$. Then $P^n = (P^m)^k P^r$. Since $\Pi$ has identical rows, $\Pi P^r = \Pi$, so:&lt;/p&gt;

\[\lVert P^n-\Pi\rVert = \lVert(P^{mk}-\Pi)P^r\rVert \leq \lVert P^{mk}-\Pi\rVert= O\!\left((1-\delta)^k\right).\]

&lt;h2 id=&quot;generalizations-and-doeblins-condition&quot;&gt;Generalizations and Doeblin’s Condition&lt;/h2&gt;

&lt;p&gt;The convergence analysis relies entirely on the decomposition $P^m = \delta \Lambda + (1 - \delta) Q$. This decomposition admits a natural probabilistic interpretation and extends to several broader settings.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Probabilistic Interpretation (Coupling).&lt;/strong&gt; At each block of $m$ steps, with probability $\delta$, the chain “forgets” its past and transitions according to the memoryless kernel $\Lambda$; with probability $1-\delta$, it follows the residual kernel $Q$. The number of blocks until the first “reset” is geometric with parameter $\delta$, which directly gives the exponential mixing rate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Non-Stationary Transitions.&lt;/strong&gt;
Transition matrices $P(1), P(2), \ldots$ may vary over time, so we do &lt;strong&gt;not&lt;/strong&gt; expect a stationary equilibrium. However, if a &lt;strong&gt;uniform lower bound&lt;/strong&gt; holds:&lt;/p&gt;

\[P(i, x, y) \geq \delta \lambda(y), \quad \forall\, x \in S,\; i \geq 1,\]

&lt;p&gt;then each $P(i) = \delta \Lambda + (1 - \delta) Q_i$, and the same telescoping argument applies:&lt;/p&gt;

\[\begin{aligned}
P(1)P(2) &amp;amp;= P(1)\bigl(\delta \Lambda + (1 - \delta)Q_2\bigr) = \delta \Lambda + (1 - \delta) P(1) Q_2 \\
&amp;amp;= \delta \Lambda + (1 - \delta)\delta \Lambda Q_2 + (1 - \delta)^2 Q_1 Q_2.
\end{aligned}\]

&lt;p&gt;In general, $\lVert P(1) \cdots P(n) - \text{(rank-1)}\rVert \to 0$ at a geometric rate, even without stationarity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Infinite Discrete State Space.&lt;/strong&gt;
For $\lvert S\rvert = \infty$, Lemma 1 may fail (see discussion below), so the decomposition $P^m \gg 0$ cannot be guaranteed. One must instead directly assume &lt;strong&gt;Doeblin’s condition&lt;/strong&gt;: there exist $m$, $\delta &amp;gt; 0$, and a probability measure $\lambda$ such that $P^m(x, \cdot) \geq \delta \lambda(\cdot)$ for all $x \in S$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Continuous State Space.&lt;/strong&gt;
Doeblin’s condition generalizes naturally: assume there exist $m$, $\delta &amp;gt; 0$, and a probability measure $\lambda$ on $(S, \mathcal{B})$ such that&lt;/p&gt;

\[\mathbb{P}_x(X_m \in B) \geq \delta \lambda(B), \quad \forall\, x \in S,\; B \in \mathcal{B}.\]

&lt;p&gt;Then $\lvert\mathbb{P}_x(X_n \in B) - \pi(B)\rvert = O(r^n)$ for some $r &amp;lt; 1$, where $\pi$ is the unique stationary measure.&lt;/p&gt;

&lt;h2 id=&quot;discussion-on-lemma-1&quot;&gt;Discussion on Lemma 1&lt;/h2&gt;

&lt;p&gt;Recall Lemma 1: &lt;em&gt;$P$ is irreducible and aperiodic on a finite state space if and only if $\exists\, m$ such that $P^m(x,y) &amp;gt; 0$ for all $x,y$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This equivalence relies critically on &lt;strong&gt;finiteness&lt;/strong&gt; of the state space. For infinite state spaces, an irreducible aperiodic chain may have no $m$ for which $P^m \gg 0$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Counterexample: Birth-death chain.&lt;/strong&gt;
Consider the chain on $\lbrace 0, 1, 2, \ldots\rbrace$ with nearest-neighbor transitions. Starting from state $s$, in any finite number of steps $m$, the chain can reach at most state $s + m$, so $P^m(s, s+m+1) = 0$ for all $m$. Hence no single $m$ makes $P^m$ uniformly positive, even though the chain is irreducible and aperiodic.&lt;/p&gt;

&lt;p&gt;This is precisely why Doeblin’s condition must be imposed as an additional assumption in the infinite state space setting.&lt;/p&gt;
</description>
        <pubDate>Fri, 06 Feb 2026 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/2026/02/06/Asymptotic-Memoryless/</link>
        <guid isPermaLink="true">http://localhost:4000/2026/02/06/Asymptotic-Memoryless/</guid>
        
        <category>Lecture Notes</category>
        
        
      </item>
    
      <item>
        <title>A Step-by-step Derivation of ADMM from DRS</title>
        <description>&lt;p&gt;In this note, we present a step-by-step derivation of the Alternating Direction Method of Multipliers (ADMM) from Douglas-Rachford Splitting (DRS). This derivation is adapted from the book below and fills in the intermediate steps that were skipped in the original exposition.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Reference: &lt;a href=&quot;https://link.springer.com/content/pdf/10.1007/978-981-16-9840-8_2&quot;&gt;Convex Optimization: Algorithms and Complexity&lt;/a&gt;, Chapter 2.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;problem-setup&quot;&gt;Problem Setup&lt;/h2&gt;

&lt;p&gt;Consider using ADMM to solve the following linearly constrained problem:&lt;/p&gt;

\[\min_{x, y} \; f(x) + g(y) \quad \text{s.t.} \; Ax + By = b\]

&lt;p&gt;The standard ADMM update consists of alternating minimization over the primal variables followed by a dual ascent step:&lt;/p&gt;

\[\begin{aligned}
x^{k+1} &amp;amp;= \arg\min_x \left\{ f(x) + \langle v^k, Ax \rangle + \tfrac{\beta}{2} \| Ax + By^k - b \|^2 \right\} \\
y^{k+1} &amp;amp;= \arg\min_y \left\{ g(y) + \langle v^k, By \rangle + \tfrac{\beta}{2} \| Ax^{k+1} + By - b \|^2 \right\} \\
v^{k+1} &amp;amp;= v^k + \beta (Ax^{k+1} + By^{k+1} - b)
\end{aligned}\]

&lt;p&gt;Our goal in this note is to derive the ADMM update from DRS applied to a suitable reformulation of the problem. To set the stage, consider the Fenchel dual problem:&lt;/p&gt;

\[\min_{\lambda} \; \underbrace{f^{\ast}(-A^{\top} \lambda) + b^{\top} \lambda}_{\varphi_1(\lambda)} + \underbrace{g^{\ast}(-B^{\top} \lambda)}_{\varphi_2(\lambda)}\]

&lt;p&gt;This is an unconstrained minimization of the sum of two convex functions $\varphi_1$ and $\varphi_2$, which is precisely the form to which DRS applies. Applying DRS to the dual problem yields the following iteration:&lt;/p&gt;

\[\begin{aligned}
v^k &amp;amp;= \operatorname{prox}_{\beta \varphi_2}(y^k) \\
u^{k+1} &amp;amp;= \operatorname{prox}_{\beta \varphi_1}(2v^k - y^k) \\
y^{k+1} &amp;amp;= y^k + u^{k+1} - v^k
\end{aligned}\]

&lt;h2 id=&quot;switched-drs&quot;&gt;Switched DRS&lt;/h2&gt;

&lt;p&gt;To connect DRS with ADMM, we first rewrite the iteration in a more convenient form. Consider the start of the next DRS iteration:&lt;/p&gt;

\[v^{k+1} = \operatorname{prox}_{\beta \varphi_2}(y^{k+1}) = \operatorname{prox}_{\beta \varphi_2}(y^k + u^{k+1} - v^k)\]

&lt;p&gt;Then consider the DRS iteration $(u^{k+1}, y^{k+1}, v^{k+1})$ and switch the order of the $y^{k+1}$ and $v^{k+1}$ updates. Since $v^{k+1}$ depends on $y^{k+1}$ only through $y^k + u^{k+1} - v^k$, which is already determined before the switch, we derive an equivalent algorithm:&lt;/p&gt;

\[\begin{aligned}
u^{k+1} &amp;amp;= \operatorname{prox}_{\beta \varphi_1}(2v^k - y^k) \\
v^{k+1} &amp;amp;= \operatorname{prox}_{\beta \varphi_2}(y^k + u^{k+1} - v^k) \\
y^{k+1} &amp;amp;= y^k + u^{k+1} - v^k.
\end{aligned}\]

&lt;p&gt;Next, apply the change of variable $w^k := v^k - y^k$ to simplify the expressions. Substituting into each line gives:&lt;/p&gt;

\[\begin{aligned}
u^{k+1} &amp;amp;= \operatorname{prox}_{\beta \varphi_1}(v^k + w^k) \\
v^{k+1} &amp;amp;= \operatorname{prox}_{\beta \varphi_2}(u^{k+1} - w^k) \\
w^{k+1} &amp;amp;= w^k + v^{k+1} - u^{k+1}
\end{aligned}\]

&lt;p&gt;This is the form of DRS from which we will recover the ADMM updates.&lt;/p&gt;

&lt;h2 id=&quot;recovering-admm&quot;&gt;Recovering ADMM&lt;/h2&gt;

&lt;p&gt;We now show that the switched DRS iteration above, applied to the dual problem, is equivalent to ADMM on the original primal problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1: optimality condition of the $u$-update.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Consider the optimality condition of $u^{k+1} = \operatorname{prox}_{\beta \varphi_1}(v^k + w^k)$. Recalling that $\varphi_1(\lambda) := f^{\ast}(-A^{\top} \lambda) + b^{\top} \lambda$, the proximal optimality condition reads:&lt;/p&gt;

\[\begin{aligned}
0 &amp;amp;\in \partial \varphi_1(u^{k+1}) + \tfrac{1}{\beta}(u^{k+1} - (v^k + w^k)) \\
&amp;amp;= -A \, \partial f^{\ast}(-A^{\top} u^{k+1}) + b + \tfrac{1}{\beta}(u^{k+1} - (v^k + w^k))
\end{aligned}\]

&lt;p&gt;Then there exists $x^{k+1} \in \partial f^{\ast}(-A^{\top} u^{k+1})$, which by the conjugate subgradient relation implies $-A^{\top} u^{k+1} \in \partial f(x^{k+1})$, such that&lt;/p&gt;

\[\begin{aligned}
0 &amp;amp;= -Ax^{k+1} + b + \tfrac{1}{\beta}(u^{k+1} - (v^k + w^k)) \\
(\Rightarrow) \quad u^{k+1} &amp;amp;= v^k + w^k + \beta(Ax^{k+1} - b)
\end{aligned} \tag{1} \label{eq:u-update}\]

&lt;p&gt;By $0 \in \partial f(x^{k+1}) + A^{\top} u^{k+1}$ and $\eqref{eq:u-update}$, we obtain&lt;/p&gt;

\[0 \in \partial f(x^{k+1}) + A^{\top}(v^k + w^k) + \beta A^{\top}(Ax^{k+1} - b) \tag{2} \label{eq:x-opt}\]

&lt;p&gt;&lt;strong&gt;Step 2: optimality condition of the $v$-update.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Consider the optimality condition of $v^{k+1} = \operatorname{prox}_{\beta \varphi_2}(u^{k+1} - w^k)$. Since $\varphi_2(\lambda) := g^{\ast}(-B^{\top} \lambda)$, we have&lt;/p&gt;

\[\begin{aligned}
0 &amp;amp;\in \partial \varphi_2(v^{k+1}) + \tfrac{1}{\beta}(v^{k+1} - (u^{k+1} - w^k)) \\
&amp;amp;= -B \, \partial g^{\ast}(-B^{\top} v^{k+1}) + \tfrac{1}{\beta}(v^{k+1} - u^{k+1} + w^k)
\end{aligned}\]

&lt;p&gt;Then there exists $y^{k+1} \in \partial g^{\ast}(-B^{\top} v^{k+1})$, which implies $-B^{\top} v^{k+1} \in \partial g(y^{k+1})$, such that&lt;/p&gt;

\[\begin{aligned}
0 &amp;amp;= -By^{k+1} + \tfrac{1}{\beta}(v^{k+1} - u^{k+1} + w^k) \\
(\Rightarrow) \quad v^{k+1} &amp;amp;= u^{k+1} - w^k + \beta B y^{k+1}
\end{aligned} \tag{3} \label{eq:v-update}\]

&lt;p&gt;From the update rule $w^{k+1} = w^k + v^{k+1} - u^{k+1}$, substituting $\eqref{eq:v-update}$ gives a useful identity:&lt;/p&gt;

\[0 = -By^{k+1} + \tfrac{1}{\beta} w^{k+1} \tag{4} \label{eq:w-identity}\]

&lt;p&gt;By $0 \in \partial g(y^{k+1}) + B^{\top} v^{k+1}$ and $\eqref{eq:v-update}$, we get&lt;/p&gt;

\[0 \in \partial g(y^{k+1}) + B^{\top}(u^{k+1} - w^k) + \beta B^{\top} B y^{k+1} \tag{5} \label{eq:y-opt}\]

&lt;p&gt;&lt;strong&gt;Step 3: Recover ADMM updates.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are now ready to piece everything together. From $\eqref{eq:x-opt}$ and $\eqref{eq:w-identity}$, note that $w^k = \beta B y^k$ by applying $\eqref{eq:w-identity}$ at iteration $k$. Substituting into $\eqref{eq:x-opt}$:&lt;/p&gt;

\[0 \in \partial f(x^{k+1}) + A^{\top} v^k + \beta A^{\top}(Ax^{k+1} - b + By^k),\]

&lt;p&gt;which is precisely the optimality condition of the $x$-update of ADMM:&lt;/p&gt;

\[x^{k+1} = \arg\min_x \left\{ f(x) + \langle v^k, Ax \rangle + \tfrac{\beta}{2} \| Ax + By^k - b \|^2 \right\}.\]

&lt;p&gt;From $\eqref{eq:y-opt}$ and $\eqref{eq:w-identity}$, substituting $u^{k+1} - w^k + \beta B y^{k+1} = v^{k+1}$ from $\eqref{eq:v-update}$ gives:&lt;/p&gt;

\[\begin{aligned}
0 &amp;amp;\in \partial g(y^{k+1}) + B^{\top}(u^{k+1} - w^k + \beta B y^{k+1}) \\
&amp;amp;= \partial g(y^{k+1}) + B^{\top}(v^k + \beta(Ax^{k+1} + By^{k+1} - b))
\end{aligned} \tag{6} \label{eq:y-admm}\]

&lt;p&gt;where the second equality follows from $\eqref{eq:u-update}$. This is exactly the optimality condition of the $y$-update of ADMM:&lt;/p&gt;

\[y^{k+1} = \arg\min_y \left\{ g(y) + \langle v^k, By \rangle + \tfrac{\beta}{2} \| Ax^{k+1} + By - b \|^2 \right\}.\]

&lt;p&gt;Finally, from $\eqref{eq:v-update}$ and $\eqref{eq:y-admm}$, we recover the dual update of ADMM:&lt;/p&gt;

\[\begin{aligned}
v^{k+1} &amp;amp;= u^{k+1} - w^k + \beta B y^{k+1} \\
&amp;amp;= v^k + \beta(Ax^{k+1} + By^{k+1} - b).
\end{aligned}\]

&lt;p&gt;This completes the derivation: the switched DRS applied to the Fenchel dual is equivalent to ADMM on the primal problem.&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Feb 2026 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/2026/02/05/From-DRS-to-ADMM/</link>
        <guid isPermaLink="true">http://localhost:4000/2026/02/05/From-DRS-to-ADMM/</guid>
        
        <category>Optimization</category>
        
        
      </item>
    
      <item>
        <title>Canyons &amp; Colors of the Southwest</title>
        <description>&lt;h2 id=&quot;grand-canyon&quot;&gt;Grand Canyon&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/img/vegas/canyon.jpeg&quot; alt=&quot;Grand Canyon&quot; style=&quot;width: 90%; height: auto;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;antelope-canyon&quot;&gt;Antelope Canyon&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/img/vegas/antelope.jpeg&quot; alt=&quot;Grand Canyon&quot; style=&quot;width: 90%; height: auto;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;bryce-canyon&quot;&gt;Bryce Canyon&lt;/h2&gt;

&lt;figure&gt;
  &lt;div style=&quot;display: flex; justify-content: center; gap: 10px;&quot;&gt;
    &lt;img src=&quot;/img/vegas/bryce.jpeg&quot; alt=&quot;Bryce Canyon&quot; style=&quot;max-width: 45%; height: auto;&quot; /&gt;
    &lt;img src=&quot;/img/vegas/bryce2.jpeg&quot; alt=&quot;Bryce Canyon&quot; style=&quot;max-width: 45%; height: auto;&quot; /&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;zion-national-park&quot;&gt;Zion National Park&lt;/h2&gt;

&lt;figure&gt;
  &lt;div style=&quot;display: flex; justify-content: center; gap: 10px;&quot;&gt;
    &lt;img src=&quot;/img/vegas/zion.jpeg&quot; alt=&quot;Zion National Park&quot; style=&quot;max-width: 45%; height: auto;&quot; /&gt;
    &lt;img src=&quot;/img/vegas/zion2.jpeg&quot; alt=&quot;Zion National Park&quot; style=&quot;max-width: 45%; height: auto;&quot; /&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;lake-powell&quot;&gt;Lake Powell&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/img/vegas/powell.jpeg&quot; alt=&quot;Lake Powell&quot; style=&quot;width: 90%; height: auto;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Dec 2025 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/2025/12/25/vegas/</link>
        <guid isPermaLink="true">http://localhost:4000/2025/12/25/vegas/</guid>
        
        <category>Travel✈️</category>
        
        
      </item>
    
      <item>
        <title>Generating Lyapunov Functions for Gradient Descent by SDP</title>
        <description>&lt;p&gt;This blog is the reading note of the following paper:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[1] Taylor, Adrien, Bryan Van Scoy, and Laurent Lessard. ‘‘Lyapunov functions for first-order methods: Tight automated convergence guarantees.” International Conference on Machine Learning. PMLR, 2018.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;A key technique for proving the convergence of optimization algorithms is the  use of a Lyapunov function (also called a potential function). Such a function is designed to decrease (or contract) at every iteration, thereby establishing the algorithm’s convergence rate. However, constructing a suitable Lyapunov function is often nontrivial and typically requires expertise.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main idea of the paper.&lt;/strong&gt; Paper [1] is among the earliest works on &lt;em&gt;automated&lt;/em&gt; Lyapunov function construction. It studies first-order methods (FOMs), including gradient descent, the heavy-ball method, and Nesterov’s accelerated gradient method, for minimizing $L$-smooth and $\mu$-strongly convex functions. The authors propose a semidefinite programming (SDP) formulation that automatically generates Lyapunov functions and provides linear convergence guarantees.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scope of this blog.&lt;/strong&gt;  The paper presents the SDP formulation directly followed by proof of its equivalence to finding a valid Lyapunov function. However, I found the derivation of the SDP formulation somewhat non-intuitive. To better understand it, I attempted to reconstruct the derivation in the simplest setting: gradient descent.&lt;/p&gt;

\[x_1 = x_0 - \alpha \nabla f (x_0)\]

&lt;p&gt;&lt;strong&gt;About potential functions.&lt;/strong&gt; The potential function is defined on the state of the algorithm,&lt;/p&gt;

\[\xi_k = (\mathbf{x}_k, \mathbf{g}_k, \mathbf{f}_k) := (x_k - x_{\star},
   \nabla f (x_k), f (x_k) - f (x_{\star})),\]

&lt;p&gt;and $\xi_{\star} = (0, 0, 0)$. The paper considers quadratic potential functions of the form&lt;/p&gt;

\[\mathcal{V} (\xi_k) = \left[ \begin{array}{l}
     \mathbf{x}_k\\
     \mathbf{g}_k
   \end{array} \right]^{\top} (P \otimes I_d) \left[ \begin{array}{l}
     \mathbf{x}_k\\
     \mathbf{g}_k
   \end{array} \right] + q \mathbf{f}_k,\]

&lt;p&gt;where $P \in \mathbb{S}^2, p \in \mathbb{R}$ are the coefficients to be determined, and $I_d$ denotes the $d \times d$ identity matrix. To prove linear convergence for gradient descent with rate $0 \leq \rho &amp;lt; 1$, the potential function must satisfy:&lt;/p&gt;

&lt;p&gt;(1) $\mathcal{V} (\xi) \geq 0$ for all $\xi$,&lt;/p&gt;

&lt;p&gt;(2) $\mathcal{V} (\xi) = 0$ if and only if $\xi = \xi_{\star}$,&lt;/p&gt;

&lt;p&gt;(3) $\mathcal{V} (\xi) \rightarrow \infty$ as $| \xi | \rightarrow \infty$,&lt;/p&gt;

&lt;p&gt;(4)  $\mathcal{V} (\xi_{k + 1}) \leq \rho^2  \mathcal{V} (\xi_k)$ for all $k$.&lt;/p&gt;

&lt;p&gt;Under these conditions, linear convergence follows:&lt;/p&gt;

\[\| x_k - x_{\star} \| = \mathcal{O} (\rho^k), \quad \| \nabla f (x_k) \| =
   \mathcal{O} (\rho^k), \quad f (x_k) - f_{\star} = \mathcal{O} (\rho^{2 k})
   .\]

&lt;h2 id=&quot;given-rate-rho-find-the-potential-functions&quot;&gt;Given rate $\rho$, find the potential functions&lt;/h2&gt;

&lt;p&gt;The problem is to find a feasible $\mathcal{V} (\xi)$. Formally,&lt;/p&gt;

\[\text{find }P, p, \quad \text{subject to conditions (1)-(4) being satisfied}\]

&lt;h3 id=&quot;translating-condition-4&quot;&gt;Translating condition (4)&lt;/h3&gt;

&lt;p&gt;Condition (4) requires,&lt;/p&gt;

\[\left[ \begin{array}{l}
     \mathbf{x}_1\\
     \mathbf{g}_1
   \end{array} \right]^{\top} (P \otimes I_d) \left[ \begin{array}{l}
     \mathbf{x}_1\\
     \mathbf{g}_1
   \end{array} \right] + q \mathbf{f}_1 \leq \rho^2 \left[ \begin{array}{l}
     \mathbf{x}_0\\
     \mathbf{g}_0
   \end{array} \right]^{\top} (P \otimes I_d) \left[ \begin{array}{l}
     \mathbf{x}_0\\
     \mathbf{g}_0
   \end{array} \right] + \rho^2 q \mathbf{f}_0.\]

&lt;p&gt;Since the left- and right-hand sides are defined over different variables, we rewrite the inequality by introducing the stacked vector&lt;/p&gt;

\[\mathbf{b} =
\begin{bmatrix}
\mathbf{x}_0 \\
\mathbf{g}_0 \\
\mathbf{g}_1 \\
f_0 \\
f_1
\end{bmatrix}.\]

&lt;p&gt;With appropriate selector matrices, we can write&lt;/p&gt;

\[\begin{aligned}
\begin{bmatrix}\mathbf{x}_1 \\ \mathbf{g}_1 \end{bmatrix}
&amp;amp;= \Bigl( \begin{bmatrix} I^x_1 \\ I^g_1 \end{bmatrix} \otimes I_d \Bigr)\mathbf{b}, 
&amp;amp;\quad f_1 &amp;amp;= I^f_1 \mathbf{b}, \\
\begin{bmatrix}\mathbf{x}_0 \\ \mathbf{g}_0 \end{bmatrix}
&amp;amp;= \Bigl( \begin{bmatrix} I^x_0 \\ I^g_0 \end{bmatrix} \otimes I_d \Bigr)\mathbf{b}, 
&amp;amp;\quad f_0 &amp;amp;= I^f_0 \mathbf{b}.
\end{aligned}\]

&lt;p&gt;Therefore, condition (4) is equivalent to&lt;/p&gt;

\[\mathbf{b}^{\top}
\Biggl(
\Bigl(
\rho^2 
\begin{bmatrix} I^x_0 \\ I^g_0 \end{bmatrix}^{\top} 
P 
\begin{bmatrix} I^x_0 \\ I^g_0 \end{bmatrix}
-
\begin{bmatrix} I^x_1 \\ I^g_1 \end{bmatrix}^{\top} 
P 
\begin{bmatrix} I^x_1 \\ I^g_1 \end{bmatrix}
\Bigr)\otimes I_d
\Biggr)\mathbf{b}
+ (\rho^2 q I^f_0 - q I^f_1)\mathbf{b}
\geq 0.
\tag{C4}\]

&lt;p&gt;&lt;strong&gt;Using interpolation conditions.&lt;/strong&gt;  Inequality (C4) is difficult to verify directly. However, it becomes tractable once we use the interpolation properties of the function class. So far, we have not exploited the fact that the points $(x_0,g_0,f_0)$ and $(x_1,g_1,f_1)$ must lie on the same $L$-smooth, $\mu$-strongly convex function.&lt;/p&gt;

&lt;p&gt;From interpolation theory, the following are equivalent:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There exists $f \in \mathcal{F}_{\mu,L}$ such that&lt;/p&gt;

\[g_k = \nabla f(x_k), \quad f_k = f(x_k), \quad \forall k \in \{0,1\};\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For all $i,j \in {0,1,\star}$, the quadratic inequalities hold:&lt;/p&gt;

\[\phi_{ij} := (L-\mu)(f_i - f_j)
+
\begin{bmatrix}
x_i \\ x_j \\ g_i \\ g_j
\end{bmatrix}^{\top}
(M^1 \otimes I_d)
\begin{bmatrix}
x_i \\ x_j \\ g_i \\ g_j
\end{bmatrix}
\geq 0,
\tag{I}\]

    &lt;p&gt;where $M^1 \in \mathbb{S}^4$ is a fixed symmetric matrix depending only on $L,\mu$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(I) can be expressed in terms of $\mathbf{b}$ as&lt;/p&gt;

\[\mathbf{b}^{\top}
\Bigl(
\underbrace{
\begin{bmatrix} I^x_i \\ I^x_j \\ I^g_i \\ I^g_j \end{bmatrix}^{\top}
M^1
\begin{bmatrix} I^x_i \\ I^x_j \\ I^g_i \\ I^g_j \end{bmatrix}
}_{M^1_{ij}}
\otimes I_d
\Bigr)\mathbf{b}
+
\underbrace{(L-\mu)(I^f_i - I^f_j)}_{m_{ij}}\mathbf{b} \geq 0.\]

&lt;p&gt;&lt;strong&gt;The SDP formulation.&lt;/strong&gt; Paper [1] shows that inequality (C) holds if and only if there exist nonnegative multipliers $\eta_{ij} \geq 0$ such that&lt;/p&gt;

\[\begin{aligned}
\sum_{i,j \in \mathcal{I}} \eta_{ij} M^1_{ij}
&amp;amp;\;\preceq\;
\rho^2
\begin{bmatrix} I^x_0 \\ I^g_0 \end{bmatrix}^{\top} P \begin{bmatrix} I^x_0 \\ I^g_0 \end{bmatrix}
-
\begin{bmatrix} I^x_1 \\ I^g_1 \end{bmatrix}^{\top} P \begin{bmatrix} I^x_1 \\ I^g_1 \end{bmatrix}, 
\\
\sum_{i,j \in \mathcal{I}} \eta_{ij} m_{ij}
&amp;amp;\;\leq\;
\rho^2 q I^f_0 - q I^f_1.
\end{aligned} \tag{SDP-1}\]

&lt;h3 id=&quot;translating-condition-1&quot;&gt;Translating condition (1)&lt;/h3&gt;

&lt;p&gt;Condition (1) requires&lt;/p&gt;

\[\begin{bmatrix}
\mathbf{x}_0 \\
\mathbf{g}_0
\end{bmatrix}^{\top}
(P \otimes I_d)
\begin{bmatrix}
\mathbf{x}_0 \\
\mathbf{g}_0
\end{bmatrix}
+ q f_0 \;\;\geq\; 0,
\quad \forall\, \mathbf{x}_0, \mathbf{g}_0, f_0.
\tag{C1}\]

&lt;p&gt;Using interpolation between $(x_0, g_0, f_0)$ and the reference point $(x_{\star}, g_{\star}, f_{\star}) = (0,0,0)$, we have&lt;/p&gt;

\[\phi := (L-\mu)\, f_0 
+ 
\begin{bmatrix}
\mathbf{x}_0 \\
\mathbf{g}_0
\end{bmatrix}^{\top}
(M^0 \otimes I_d)
\begin{bmatrix}
\mathbf{x}_0 \\
\mathbf{g}_0
\end{bmatrix}
\;\;\geq 0,\]

&lt;p&gt;where $M^0 \in \mathbb{S}^2$ is a fixed symmetric matrix depending only on $L,\mu$.&lt;/p&gt;

&lt;p&gt;Therefore, inequality (C1) holds if there exists $\lambda \geq 0$ such that&lt;/p&gt;

\[\begin{aligned}
\lambda M^0 &amp;amp;\;\preceq\; P,  \\
\lambda (L-\mu) &amp;amp;\;\leq\; q. 
\end{aligned}\tag{SDP-2}\]

&lt;p&gt;Conditions (2) and (3) are automatically ensured when (SDP-1) and (SDP-2) hold.&lt;/p&gt;

&lt;h3 id=&quot;the-complete-sdp-formulation&quot;&gt;The complete SDP formulation&lt;/h3&gt;

&lt;p&gt;Putting everything together: given contraction ratio $\rho$, the SDP problem to find a quadratic potential function is&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;($\rho$-SDP)&lt;/strong&gt;: Find $P \in \mathbb{S}^2$, $q \in \mathbb{R}$, multipliers $\eta_{ij} \geq 0$ for $i,j \in {0,1,\star}$, and $\lambda \geq 0$ such that&lt;/p&gt;

\[\begin{aligned}
0 &amp;amp;\;\preceq\; 
\rho^2 
\begin{bmatrix} I^x_0 \\ I^g_0 \end{bmatrix}^{\top} 
P 
\begin{bmatrix} I^x_0 \\ I^g_0 \end{bmatrix}
-
\begin{bmatrix} I^x_1 \\ I^g_1 \end{bmatrix}^{\top} 
P 
\begin{bmatrix} I^x_1 \\ I^g_1 \end{bmatrix}
- \sum_{i,j \in \mathcal{I}} \eta_{ij} M^1_{ij}, \\
0 &amp;amp;\;\leq\; (\rho^2 q I^f_0 - q I^f_1) - \sum_{i,j \in \mathcal{I}} \eta_{ij} m_{ij}, \\
0 &amp;amp;\;\preceq\; P - \lambda M^0, \\
0 &amp;amp;\;\leq\; q - \lambda (L-\mu).
\end{aligned}\]

&lt;p&gt;Paper [1] proves that a quadratic potential function with contraction rate $\rho$ exists if and only if this SDP is feasible.&lt;/p&gt;

&lt;h2 id=&quot;find-rho&quot;&gt;Find $\rho$&lt;/h2&gt;

&lt;p&gt;Given $\rho$, we solve &lt;strong&gt;($\rho$-SDP)&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If the problem is feasible, the algorithm admits a Lyapunov function with contraction rate at most $\rho$.&lt;/li&gt;
  &lt;li&gt;If the problem is infeasible, the algorithm cannot contract at rate $\rho$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To obtain the tightest convergence guarantee, we perform a bisection search on the interval $[0,1]$. Starting from the midpoint, we test feasibility:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;if feasible, we continue the search on the left subinterval;&lt;/li&gt;
  &lt;li&gt;if infeasible, we continue on the right subinterval.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The procedure terminates when we identify the smallest $\rho$ for which &lt;strong&gt;($\rho$-SDP)&lt;/strong&gt; is feasible.&lt;/p&gt;

</description>
        <pubDate>Sat, 06 Sep 2025 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2025/09/06/AutoLyap/</link>
        <guid isPermaLink="true">http://localhost:4000/2025/09/06/AutoLyap/</guid>
        
        <category>Optimization</category>
        
        
      </item>
    
      <item>
        <title>Hangzhou, Yichang, and Xi&apos;an</title>
        <description>&lt;h2 id=&quot;hangzhou&quot;&gt;Hangzhou&lt;/h2&gt;

&lt;p&gt;On May 10, I took a one-day trip with my family to Hangzhou, where we celebrated a special Mother’s Day by visiting West Lake and Lingyin Temple.&lt;/p&gt;

&lt;figure&gt;
  &lt;div style=&quot;display: flex; justify-content: center; gap: 10px;&quot;&gt;
    &lt;img src=&quot;/img/Hangzhou/1.jpeg&quot; alt=&quot;Subfigure 1&quot; style=&quot;max-width: 45%; height: auto;&quot; /&gt;
    &lt;img src=&quot;/img/Hangzhou/2.jpeg&quot; alt=&quot;Subfigure 2&quot; style=&quot;max-width: 45%; height: auto;&quot; /&gt;
  &lt;/div&gt;
  &lt;figcaption style=&quot;text-align: center; margin-top: 8px;&quot;&gt;
    West Lake at sunset.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;yichang&quot;&gt;Yichang&lt;/h2&gt;

&lt;p&gt;In April, my friends and I went on a graduation trip to Yichang. Beyond the beautiful scenery, what touched me most was the hospitality of the local people. I hope I can share the same warmth with others I meet in the future. The following photos were taken by Chuwen.&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/img/Yichang/1.jpeg&quot; alt=&quot;test&quot; style=&quot;width: 90%; height: auto;&quot; /&gt;
  &lt;figcaption style=&quot;text-align: center; margin-top: 8px;&quot;&gt;A tea garden in the rain.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/img/Yichang/3.jpeg&quot; alt=&quot;test&quot; style=&quot;width: 90%; height: auto;&quot; /&gt;
  &lt;figcaption style=&quot;text-align: center; margin-top: 8px;&quot;&gt;Qingjiang River 清江画廊&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;xian&quot;&gt;Xi’an&lt;/h2&gt;

&lt;p&gt;During the graduation break, I traveled to Xi’an with my sisters. Among all the sights we visited, my favorite was the lively pedestrian street beneath the city wall—where I also picked up a beautiful fan!&lt;/p&gt;

&lt;figure&gt;
  &lt;div style=&quot;display: flex; justify-content: center; gap: 10px;&quot;&gt;
    &lt;img src=&quot;/img/Xian/1.jpeg&quot; alt=&quot;Subfigure 1&quot; style=&quot;max-width: 45%; height: auto;&quot; /&gt;
    &lt;img src=&quot;/img/Xian/2.jpeg&quot; alt=&quot;Subfigure 2&quot; style=&quot;max-width: 45%; height: auto;&quot; /&gt;
  &lt;/div&gt;
  &lt;figcaption style=&quot;text-align: center; margin-top: 8px;&quot;&gt;
    书院门步行街里的店铺，和我的扇子
  &lt;/figcaption&gt;
&lt;/figure&gt;
</description>
        <pubDate>Fri, 01 Aug 2025 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2025/08/01/HangZhou/</link>
        <guid isPermaLink="true">http://localhost:4000/2025/08/01/HangZhou/</guid>
        
        <category>Travel✈️</category>
        
        
      </item>
    
      <item>
        <title>Reflections from My First Academic Talk</title>
        <description>&lt;p&gt;I just gave my first academic talk at a conference at SJTU IIC yesterday. I presented an ongoing project, and I had hesitated for a long time about whether to give a talk on an “incomplete” work. It turns out much of my nervousness was unnecessary—I received a lot of helpful feedback from the audience. So I’m writing this post to summarize some lessons I’ve learned during this journey. Many of them echo the insights from &lt;em&gt;Paths to Research&lt;/em&gt; by Christopher Ryan and Runshan Fu (which I highly recommend!): &lt;a href=&quot;https://christopher-thomas-ryan.github.io/papers/Paths_to_Research.pdf&quot;&gt;Paths to Research (PDF)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;be-confident&quot;&gt;Be confident&lt;/h2&gt;

&lt;p&gt;In my first draft of the slides, I presented the results with a negative tone. I said things like “we use this &lt;em&gt;strong&lt;/em&gt; assumption” or “this technique &lt;em&gt;might&lt;/em&gt; be helpful.” I did this because I was afraid that some smart audience member would immediately identify the limitations and criticize my work, so I tried to preempt them by pointing out the weaknesses myself.&lt;/p&gt;

&lt;p&gt;It was my collaborator who reminded me to be confident—especially when presenting to others. If your entire talk focuses on what &lt;em&gt;doesn’t&lt;/em&gt; work, the audience might wonder why they’re even there. Even if someone in the room is an expert, they haven’t spent 30+ hours a week thinking about your specific research question. You are the person who knows the most about your work, and that alone is reason enough to speak with confidence.&lt;/p&gt;

&lt;h2 id=&quot;on-the-purpose-of-a-talk&quot;&gt;On the purpose of a talk&lt;/h2&gt;

&lt;p&gt;Here’s my personal understanding of what a talk is for:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It’s an advertisement for your work. You’re not there to explain every technical detail. Your goal is to highlight the most compelling parts—motivation and key results—and spark enough interest that the audience will look up your paper afterward.&lt;/li&gt;
  &lt;li&gt;It’s a chance to collect feedback. In this sense, presenting an ongoing or incomplete project is actually &lt;em&gt;better&lt;/em&gt;. Since it’s not published yet, you can still revise it freely. I got many valuable suggestions this way—it’s honestly one of the most efficient forms of peer review, better than sending a draft to other researchers.&lt;/li&gt;
  &lt;li&gt;It’s an opportunity to connect with people. Beyond the Q&amp;amp;A, audience members might approach you afterward to talk. One PhD student came up to me and said how happy she was that she finally understood an optimization talk—she works in Operations Management and usually gets lost. Her words reminded me of the importance of making my talk accessible. If someone gets lost early on, they may spend the rest of the talk feeling uncomfortable or even doubting themselves.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;prepare-for-questions&quot;&gt;Prepare for questions&lt;/h2&gt;

&lt;p&gt;I didn’t prepare for the Q&amp;amp;A at all. I figured, “It’s the last talk of the conference—who’s going to ask questions?” Big mistake. I got so nervous that I didn’t fully understand the question and gave an off-topic answer. That moment cost me the chance to explain my work more clearly and in greater depth.&lt;/p&gt;

&lt;p&gt;Luckily, I later found the questioners and had some great follow-up conversations. But I learned the hard way that anticipating possible questions—and rehearsing your answers—is absolutely worth the effort.&lt;/p&gt;

&lt;p&gt;Finally, I’m deeply grateful to my collaborator for helping me rehearse, and to everyone in the audience for being there and offering feedback. Their encouragement gave me a big boost of confidence for my future research journey.&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Jul 2025 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2025/07/07/FirstTalk/</link>
        <guid isPermaLink="true">http://localhost:4000/2025/07/07/FirstTalk/</guid>
        
        <category>Tips</category>
        
        
      </item>
    
      <item>
        <title>Helpful Resources in Grad School</title>
        <description>&lt;p&gt;I’ve benefited greatly from reading advice posts—especially during my graduate school application. In this post, I’ve collected some of the most helpful resources I’ve come across, covering both graduate school applications and research life. I will continue to update this list as I discover new and insightful advice.&lt;/p&gt;

&lt;p&gt;As a gentle disclaimer, I’ll borrow a word of caution from &lt;em&gt;Carl Sandburg&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Beware of advice, even this.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;applying-to-graduate-school&quot;&gt;Applying to Graduate School&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://csrankings.org/advice.html&quot;&gt;Advice on Applying to Grad School in Computer Science&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://matt.might.net/articles/how-to-apply-and-get-in-to-graduate-school-in-science-mathematics-engineering-or-computer-science/&quot;&gt;HOWTO: Get into grad school for science, engineering, math and computer science&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://matt.might.net/articles/how-to-recommendation-letter/&quot;&gt;How to get a great letter of recommendation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://bldavies.com/blog/applying-economics-phd-programs/#interviews&quot;&gt;Applying to economics PhD programs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cs-sop.notion.site/CS-PhD-Statements-of-Purpose-df39955313834889b7ac5411c37b958d&quot;&gt;CS PhD Statements of Purpose&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2015/10/how-to-write-a-bad-statement-for-a-computer-science-phd-admissions-application.html&quot;&gt;How to Write a Bad Statement for a Computer Science Ph.D. Admissions Application&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;giving-talks&quot;&gt;Giving Talks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://graphics.stanford.edu/~kayvonf/misc/cleartalktips.pdf&quot;&gt;Tips for Giving Clear Talks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://matt.might.net/articles/academic-presentation-tips/&quot;&gt;10 tips for academic talks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.jhu.edu/~jason/advice/how-to-give-a-talk.html&quot;&gt;How to Prepare a Talk&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://christopher-thomas-ryan.github.io/papers/Paths_to_Research.pdf&quot;&gt;(Highly Recommend!) Paths to Research, Chapter 10&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;writing-cvs&quot;&gt;Writing CVs&lt;/h2&gt;

&lt;p&gt;Tips from Course ESOLLANG 697 at Stanford:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Do &lt;em&gt;not&lt;/em&gt; include photo, race, religion, nationality, birthday, gender, etc.&lt;/li&gt;
  &lt;li&gt;Highlight the last name. If your name is Zhang San in Chinese pinyin, then use (Zhang, San / San ZHANG). If it’s scary to use all caps, use &lt;em&gt;small cap&lt;/em&gt; instead. In LaTex, it’s (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;San \textsc{Zhang}&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;Include mailing address, phone number, and email address.&lt;/li&gt;
  &lt;li&gt;Do &lt;em&gt;not&lt;/em&gt; include experience prior to college.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;writing-emails&quot;&gt;Writing Emails&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lauraportwoodstacer.com/how-to-email-your-professor-without-being-annoying-af&quot;&gt;How to Email Your Professor (without being annoying AF)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tips from Course ESOLLANG 697 at Stanford:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Start with your request, or who you are if this is your first time emailing that person, rather than “I hope this email finds you well”.&lt;/li&gt;
      &lt;li&gt;Be short.&lt;/li&gt;
      &lt;li&gt;Salutation: start with “Dear Professor [Last Name]”, and switch to “Hello [First Name]” when you have close collaboration with him/her.&lt;/li&gt;
      &lt;li&gt;The larger the request, the more polite the language. Use “I wonder”, “if you might be willing to” or past tense to create distance and politeness. Also remember to give the receiver options and not to impose.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;technical-writing&quot;&gt;Technical Writing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.ubc.ca/~schmidtm/Courses/Notes/writing.pdf&quot;&gt;Some Notes on Writing, by Prof. Mark Schmidt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;professional-research-advice&quot;&gt;Professional Research Advice&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.jhu.edu/~jason/advice/&quot;&gt;Advice for Research Students, by Prof. Jason Eisner&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://matt.might.net/articles/&quot;&gt;Blog of Prof. Matt Might&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://homes.cs.washington.edu/~mernst/advice/agre-networking-on-the-network-20050814.html&quot;&gt;Networking on the Network:  A Guide to Professional Skills for PhD Students, by Prof. Phil Agre&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://christopher-thomas-ryan.github.io/papers/Paths_to_Research.pdf&quot;&gt;Book: Paths to Research, by Christopher Thomas Ryan and Runshan Fu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;miscellaneous&quot;&gt;Miscellaneous&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A Science History Blog: &lt;a href=&quot;https://etherwave.wordpress.com/about/&quot;&gt;Ether Wave Propaganda: a history of science blog, by Will Thomas and Christopher Donohue&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tips for Project Framing: &lt;a href=&quot;https://stanfordh4d.substack.com/p/technology-transfer-for-defense-leveraging&quot;&gt;Leveraging the Heilmeier Catechism: A Blueprint for Effective Project Framing&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fun: &lt;a href=&quot;https://xkcd.com&quot;&gt;xkcd: A webcomic of romance, sarcasm, math, and language&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 22 Jun 2025 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2025/06/22/Resource/</link>
        <guid isPermaLink="true">http://localhost:4000/2025/06/22/Resource/</guid>
        
        <category>Tips</category>
        
        
      </item>
    
      <item>
        <title>Routines for Setting Up a New Server</title>
        <description>&lt;p&gt;Lately, I’ve been running deep learning experiments across different computing clusters. Every time I switch to a new server, I have to go through a series of setup steps to get my environment ready. To avoid repeating the same work from scratch each time, I decided to document my routine here. This post mainly serves as a personal checklist, but it might also be useful to others facing similar tasks. I’ll keep it updated whenever I add new steps to the routine.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[!NOTE]&lt;/p&gt;

  &lt;p&gt;[Updated 10/25] Recently, I found a useful AI tool built in the Mac Terminal - &lt;a href=&quot;https://www.warp.dev/&quot;&gt;Warp&lt;/a&gt;. I can use it to generate commands and scripts, check the environment of a new server, analyze errors, and automate pipeline. It is particularly helpful when using the remote server. So the routines in this post has been replaced by this wonderful AI tool. I also recommend you giving it a try!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;connect-to-github-account&quot;&gt;Connect to GitHub Account&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Generate an SSH key&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-keygen &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; ed25519 &lt;span class=&quot;nt&quot;&gt;-C&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;your_email@example.com&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After generating the key, display it with:&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; ~/.ssh/id_ed25519.pub
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add the public key to GitHub.&lt;/p&gt;

    &lt;p&gt;Navigate to &lt;strong&gt;Settings &amp;gt; SSH and GPG keys&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Click &lt;strong&gt;“New SSH key”&lt;/strong&gt;, then paste the copied content&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Testing SSH connection.&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh &lt;span class=&quot;nt&quot;&gt;-T&lt;/span&gt; git@github.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;If prompted, type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;yes&lt;/code&gt; and press Enter.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;set-up-a-conda-environment&quot;&gt;Set Up a Conda Environment&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Create a new environment.&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda create &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; myenv &lt;span class=&quot;nv&quot;&gt;python&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3.10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use a faster pip mirror&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip config &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
pip config &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;install.trusted-host pypi.tuna.tsinghua.edu.cn
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;set-up-hugging-face-mirror&quot;&gt;Set Up Hugging Face Mirror&lt;/h2&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HF_ENDPOINT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://hf-mirror.com&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;export HF_ENDPOINT=&quot;https://hf-mirror.com&quot;&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then reload your shell:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Fri, 23 May 2025 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2025/05/23/ServerEnv/</link>
        <guid isPermaLink="true">http://localhost:4000/2025/05/23/ServerEnv/</guid>
        
        <category>Tips</category>
        
        
      </item>
    
      <item>
        <title>Optimizing EPLB by Integer (Conic) Linear Programming</title>
        <description>&lt;p&gt;In the last post, I reviewed the code of &lt;a href=&quot;https://github.com/deepseek-ai/EPLB&quot;&gt;EPLB&lt;/a&gt; (Expert Parallelism Load Balancer). As a quick recap, EPLB is a toolbox for expert load balancing in the MoE architecture, it outputs the expert replication plan, grouping plan, and reallocation plan. The main idea of the whole algorithm is to decompose the joint decision problems into three subproblems and solve each by greedy methods.&lt;/p&gt;

&lt;p&gt;The main algorithms used are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;balanced expert replication&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;balanced packing&lt;/code&gt;. In EPLB, they are all solved by greedy algorithms (see my last post for the pseudocode). In this post, I will present how to solve these problems by optimization, in particular, by integer (conic) linear programming. Besides optimization formulation, I also implemented these algorithms in Python. The interface is the same as EPLB, and I termed it as IPLB (Integer Programming based Load Balancer). I have opensourced the algorithm on Github:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/zwyhahaha/IPLB&quot;&gt;https://github.com/zwyhahaha/IPLB&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Any feedback is welcome!&lt;/p&gt;

&lt;h2 id=&quot;function-replica_experts&quot;&gt;Function: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replica_experts&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: weights(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_layers&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_log_experts&lt;/code&gt;), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_phy_experts&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: replicate experts, for each layer, minimize the maximal workload.&lt;/p&gt;

&lt;p&gt;For each layer $l$, the weight (workload) is $w_l = (w_{l1}, …, w_{le})$, the number of physical experts (replicas) is $c$. The decision variable is the number of replicas for each logical expert, $r_l = (r_{l1}, …, r_{le})$. The optimization problem is,&lt;/p&gt;

\[\begin{array}{rcl}
  \min_{r_l} &amp;amp; \max_{1 \leq j \leq e}  \frac{w_{l
  j}}{r_{l j}} &amp;amp; \\
  \text{s.t.} &amp;amp; \sum_{j = 1}^e r_{l j} \leq c &amp;amp; \\
  &amp;amp; r_{l j} \in \mathbb{Z}_+ &amp;amp; 
\end{array}\]

&lt;p&gt;After reformulation,&lt;/p&gt;

\[\begin{array}{rcl}
  \min_{\alpha, {\mathbf{r}}_l} &amp;amp; \alpha &amp;amp; \\
  \text{s.t.} &amp;amp; w_{l j} \leq \alpha r_{l j} &amp;amp; \forall 1 \leq j \leq e\\
  &amp;amp; \sum_{j = 1}^e r_{l j} \leq c &amp;amp; \\
  &amp;amp; r_{l j} \in \mathbb{Z}_+ &amp;amp; 
\end{array}\]

&lt;p&gt;The constraint that, $w_{l j} \leq \alpha r_{l j}$ is bilinear. However, since $\alpha$ and $r_{l j}$ are both positive, this constraint is actually convex and is semidefinite cone or second-order cone.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;semidefinite cone&lt;/li&gt;
&lt;/ol&gt;

\[\left(\begin{array}{cc}
     \alpha &amp;amp; \sqrt{w_{l j}}\\
     \sqrt{w_{l j}} &amp;amp; r_{l j}
   \end{array}\right) \succcurlyeq 0\]

&lt;p&gt;which is equivalent to $\alpha \geq 0, r \geq 0,$$w_{l j} \leq \alpha r_{l 
j}$.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Second order cone.&lt;/li&gt;
&lt;/ol&gt;

\[\left(\begin{array}{c}
     \alpha\\
     r_{l j}\\
     \sqrt{2 w_{l j}}
   \end{array}\right) \in \mathcal{Q}_r^3\]

&lt;p&gt;where $\mathcal{Q}_r^3 = { (x, y, z) \in \mathbb{R}^3 : 2 x y \geq z^2, x 
\geq 0, y \geq 0 }$ represents rotated second-order cone. Equivalently, this is a second-order cone after linear transformation:&lt;/p&gt;

\[\left(\begin{array}{c}
     \alpha\\
     \frac{r_{l j} - \sqrt{2 w_{l j}}}{2}\\
     \frac{r_{l j} + \sqrt{2 w_{l j}}}{2}
   \end{array}\right) \in \mathcal{Q}^3\]

&lt;p&gt;where $\mathcal{Q}^3 = { (x, y, z) \in \mathbb{R}^3 : z^2 \geq x^2 + y^2 }$.&lt;/p&gt;

&lt;p&gt;Therefore, the problem &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replica_experts&lt;/code&gt; can be solved by &lt;em&gt;integer conic linear programming&lt;/em&gt;, which is supported by &lt;strong&gt;GUROBI&lt;/strong&gt; and &lt;strong&gt;COPT&lt;/strong&gt;!&lt;/p&gt;

&lt;h2 id=&quot;function-balanced_packing&quot;&gt;Function: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;balanced_packing&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: weights(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_layers&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_obj&lt;/code&gt;), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_packs&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: assign &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_obj&lt;/code&gt; objects to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_packs&lt;/code&gt; packs, for each layer, minimize the maximal workload among packs.&lt;/p&gt;

&lt;p&gt;For each layer $l$, the weight is ${\mathbf{w}} = (w_1, …, w_n)$. The decision variable is, the assignment from objects to packs $a_{i j}$. The optimization problem is,&lt;/p&gt;

\[\begin{array}{rcl}
  \min_{ a } &amp;amp; \max_{1 \leq j \leq m}  \sum_{i = 1}^n w_i a_{i j} &amp;amp; \\
  \text{s.t.} &amp;amp; \sum_{j = 1}^m a_{i j} = 1 &amp;amp; \\
   &amp;amp; \sum_{i = 1}^n a_{i j} \leq  \frac{n}{m} &amp;amp; \\
   &amp;amp; a_{i j} \in \{ 0, 1 \} &amp;amp; 
\end{array}\]

&lt;p&gt;After reformulation,&lt;/p&gt;

\[\begin{array}{rcl}
  \min_{a} &amp;amp; \alpha &amp;amp; \\
  \text{s.t.} &amp;amp; \alpha \geq \sum_{i = 1}^n w_i a_{i j} &amp;amp; \forall 1 \leq j \leq m\\
  &amp;amp; \sum_{j = 1}^m a_{i j} = 1 &amp;amp; \\
  &amp;amp; \sum_{i = 1}^n a_{i j} \leq  \frac{n}{m} &amp;amp; \\
  &amp;amp; a_{i j} \in \{ 0, 1 \} &amp;amp; 
\end{array}\]

&lt;p&gt;which is a standard &lt;em&gt;integer linear programming&lt;/em&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 19 May 2025 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2025/05/19/IPLB/</link>
        <guid isPermaLink="true">http://localhost:4000/2025/05/19/IPLB/</guid>
        
        <category>Optimization</category>
        
        
      </item>
    
  </channel>
</rss>
